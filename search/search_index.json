{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Concept \u00b6 Concept is a technique that leverages CLIP and BERTopic-based techniques to perform Concept Modeling on images. Since topics are part of conversations and text, they do not represent the context of images well. Therefore, these clusters of images are referred to as 'Concepts' instead of the traditional 'Topics'. Thus, Concept Modeling takes inspiration from topic modeling techniques to cluster images, find common concepts and model them both visually using images and textually using topic representations. Installation \u00b6 Installation, with sentence-transformers, can be done using pypi : pip install concept Quick Start \u00b6 First, we need to download and extract 25.000 images from Unsplash used in the sentence-transformers example: import os import zipfile from tqdm import tqdm from PIL import Image from sentence_transformers import util # 25k images from Unsplash img_folder = 'photos/' if not os . path . exists ( img_folder ) or len ( os . listdir ( img_folder )) == 0 : os . makedirs ( img_folder , exist_ok = True ) photo_filename = 'unsplash-25k-photos.zip' if not os . path . exists ( photo_filename ): #Download dataset if does not exist util . http_get ( 'http://sbert.net/datasets/' + photo_filename , photo_filename ) #Extract all images with zipfile . ZipFile ( photo_filename , 'r' ) as zf : for member in tqdm ( zf . infolist (), desc = 'Extracting' ): zf . extract ( member , img_folder ) images = [ Image . open ( \"photos/\" + filepath ) for filepath in tqdm ( img_names )] Next, we only need to pass images to Concept : from concept import ConceptModel concept_model = ConceptModel () concepts = concept_model . fit_transform ( images ) The resulting concepts can be visualized through concept_model.visualize_concepts() : However, to get the full experience, we need to label the concept clusters with topics. To do this, we need to create a vocabulary: from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] vectorizer = TfidfVectorizer ( ngram_range = ( 1 , 2 )) . fit ( docs ) words = vectorizer . get_feature_names () words = [ words [ index ] for index in np . argpartition ( vectorizer . idf_ , - 50_000 )[ - 50_000 :]] Then, we can pass in the resulting words to Concept : from concept import ConceptModel concept_model = ConceptModel () concepts = concept_model . fit_transform ( images , docs = words ) Again, the resulting concepts can be visualized. This time however, we can also see the generated topics through concept_model.visualize_concepts() : NOTE : Use Concept(embedding_model=\"clip-ViT-B-32-multilingual-v1\") to select a model that supports 50+ languages.","title":"Home"},{"location":"index.html#concept","text":"Concept is a technique that leverages CLIP and BERTopic-based techniques to perform Concept Modeling on images. Since topics are part of conversations and text, they do not represent the context of images well. Therefore, these clusters of images are referred to as 'Concepts' instead of the traditional 'Topics'. Thus, Concept Modeling takes inspiration from topic modeling techniques to cluster images, find common concepts and model them both visually using images and textually using topic representations.","title":"Concept"},{"location":"index.html#installation","text":"Installation, with sentence-transformers, can be done using pypi : pip install concept","title":"Installation"},{"location":"index.html#quick-start","text":"First, we need to download and extract 25.000 images from Unsplash used in the sentence-transformers example: import os import zipfile from tqdm import tqdm from PIL import Image from sentence_transformers import util # 25k images from Unsplash img_folder = 'photos/' if not os . path . exists ( img_folder ) or len ( os . listdir ( img_folder )) == 0 : os . makedirs ( img_folder , exist_ok = True ) photo_filename = 'unsplash-25k-photos.zip' if not os . path . exists ( photo_filename ): #Download dataset if does not exist util . http_get ( 'http://sbert.net/datasets/' + photo_filename , photo_filename ) #Extract all images with zipfile . ZipFile ( photo_filename , 'r' ) as zf : for member in tqdm ( zf . infolist (), desc = 'Extracting' ): zf . extract ( member , img_folder ) images = [ Image . open ( \"photos/\" + filepath ) for filepath in tqdm ( img_names )] Next, we only need to pass images to Concept : from concept import ConceptModel concept_model = ConceptModel () concepts = concept_model . fit_transform ( images ) The resulting concepts can be visualized through concept_model.visualize_concepts() : However, to get the full experience, we need to label the concept clusters with topics. To do this, we need to create a vocabulary: from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] vectorizer = TfidfVectorizer ( ngram_range = ( 1 , 2 )) . fit ( docs ) words = vectorizer . get_feature_names () words = [ words [ index ] for index in np . argpartition ( vectorizer . idf_ , - 50_000 )[ - 50_000 :]] Then, we can pass in the resulting words to Concept : from concept import ConceptModel concept_model = ConceptModel () concepts = concept_model . fit_transform ( images , docs = words ) Again, the resulting concepts can be visualized. This time however, we can also see the generated topics through concept_model.visualize_concepts() : NOTE : Use Concept(embedding_model=\"clip-ViT-B-32-multilingual-v1\") to select a model that supports 50+ languages.","title":"Quick Start"},{"location":"changelog.html","text":"Version 0.1.0 \u00b6 Release date: 27 October, 2021 Update Readme with small example Create documentation page: https://maartengr.github.io/Concept/ Fix fit not working properly Better visualization of resulting concepts Version 0.0.1 \u00b6 Release date: 27 October, 2021 The first release of Concept Modeling \ud83e\udd73, a technique that allows for topic modeling of images and text together.","title":"Changelog"},{"location":"changelog.html#version-010","text":"Release date: 27 October, 2021 Update Readme with small example Create documentation page: https://maartengr.github.io/Concept/ Fix fit not working properly Better visualization of resulting concepts","title":"Version 0.1.0"},{"location":"changelog.html#version-001","text":"Release date: 27 October, 2021 The first release of Concept Modeling \ud83e\udd73, a technique that allows for topic modeling of images and text together.","title":"Version 0.0.1"},{"location":"api/concept.html","text":"Concept \u00b6 Concept is a technique that leverages CLIP and BERTopic-based techniques to perform Concept Modeling on images. Since topics are part of conversations and text, they do not represent the context of images well. Therefore, these clusters of images are referred to as 'Concepts' instead of the traditional 'Topics'. Thus, Concept Modeling takes inspiration from topic modeling techniques to cluster images, find common concepts and model them both visually using images and textually using topic representations. Usage: from concept import ConceptModel concept_model = ConceptModel () concept_clusters = concept_model . fit_transform ( images ) __init__ ( self , min_concept_size = 30 , diversity = 0.3 , embedding_model = 'clip-ViT-B-32' , vectorizer_model = None , umap_model = None , hdbscan_model = None ) special \u00b6 Concept Model Initialization Parameters: Name Type Description Default min_concept_size int The minimum size of concepts. Increasing this value will lead to a lower number of concept clusters. 30 diversity float How diverse the images within a concept are. Values between 0 and 1 with 0 being not diverse at all and 1 being most diverse. 0.3 embedding_model str The CLIP model to use. Current options include: * clip-ViT-B-32 * clip-ViT-B-32-multilingual-v1 'clip-ViT-B-32' vectorizer_model CountVectorizer Pass in a CountVectorizer instead of the default None umap_model UMAP Pass in a UMAP model to be used instead of the default None hdbscan_model HDBSCAN Pass in a hdbscan.HDBSCAN model to be used instead of the default None Source code in concept\\_model.py def __init__ ( self , min_concept_size : int = 30 , diversity : float = 0.3 , embedding_model : str = \"clip-ViT-B-32\" , vectorizer_model : CountVectorizer = None , umap_model : UMAP = None , hdbscan_model : hdbscan . HDBSCAN = None ): \"\"\" Concept Model Initialization Arguments: min_concept_size: The minimum size of concepts. Increasing this value will lead to a lower number of concept clusters. diversity: How diverse the images within a concept are. Values between 0 and 1 with 0 being not diverse at all and 1 being most diverse. embedding_model: The CLIP model to use. Current options include: * clip-ViT-B-32 * clip-ViT-B-32-multilingual-v1 vectorizer_model: Pass in a CountVectorizer instead of the default umap_model: Pass in a UMAP model to be used instead of the default hdbscan_model: Pass in a hdbscan.HDBSCAN model to be used instead of the default \"\"\" self . diversity = diversity self . min_concept_size = min_concept_size # Embedding model self . embedding_model = SentenceTransformer ( embedding_model ) # Vectorizer self . vectorizer_model = vectorizer_model or CountVectorizer () # UMAP self . umap_model = umap_model or UMAP ( n_neighbors = 15 , n_components = 5 , min_dist = 0.0 , metric = 'cosine' ) # HDBSCAN self . hdbscan_model = hdbscan_model or hdbscan . HDBSCAN ( min_cluster_size = self . min_concept_size , metric = 'euclidean' , cluster_selection_method = 'eom' , prediction_data = True ) self . frequency = None self . topics = None fit ( self , images , image_names = None , image_embeddings = None ) \u00b6 Fit the model on a collection of images and return concepts Parameters: Name Type Description Default images List[PIL.Image.Image] A list of images to fit the model on required image_names List[str] The names of the images for easier reading of concept clusters None image_embeddings ndarray Pre-trained image embeddings to use instead of generating them in Concept None Usage: from concept import ConceptModel concept_model = ConceptModel () concepts = concept_model . fit ( images ) Source code in concept\\_model.py def fit ( self , images : List [ Image . Image ], image_names : List [ str ] = None , image_embeddings : np . ndarray = None ): \"\"\" Fit the model on a collection of images and return concepts Arguments: images: A list of images to fit the model on image_names: The names of the images for easier reading of concept clusters image_embeddings: Pre-trained image embeddings to use instead of generating them in Concept Usage: ```python from concept import ConceptModel concept_model = ConceptModel() concepts = concept_model.fit(images) ``` \"\"\" self . fit_transform ( images , image_names = image_names , image_embeddings = image_embeddings ) return self fit_transform ( self , images , docs = None , image_names = None , image_embeddings = None ) \u00b6 Fit the model on a collection of images and return concepts Parameters: Name Type Description Default images List[PIL.Image.Image] A list of images to fit the model on required docs List[str] The documents from which to extract textual concept representation None image_names List[str] The names of the images for easier reading of concept clusters None image_embeddings ndarray Pre-trained image embeddings to use instead of generating them in Concept None Returns: Type Description predictions Concept prediction for each image Usage: from concept import ConceptModel concept_model = ConceptModel () concepts = concept_model . fit_transform ( images ) Source code in concept\\_model.py def fit_transform ( self , images : List [ Image . Image ], docs : List [ str ] = None , image_names : List [ str ] = None , image_embeddings : np . ndarray = None ) -> List [ int ]: \"\"\" Fit the model on a collection of images and return concepts Arguments: images: A list of images to fit the model on docs: The documents from which to extract textual concept representation image_names: The names of the images for easier reading of concept clusters image_embeddings: Pre-trained image embeddings to use instead of generating them in Concept Returns: predictions: Concept prediction for each image Usage: ```python from concept import ConceptModel concept_model = ConceptModel() concepts = concept_model.fit_transform(images) ``` \"\"\" # Calculate image embeddings if not already generated if image_embeddings is None : image_embeddings = self . _embed_images ( images ) # Reduce dimensionality and cluster images into concepts reduced_embeddings = self . _reduce_dimensionality ( image_embeddings ) predictions = self . _cluster_embeddings ( reduced_embeddings ) # Extract representative images through exemplars representative_images = self . _extract_exemplars ( image_names ) cluster_embeddings , exemplar_embeddings = self . _extract_cluster_embeddings ( image_embeddings , representative_images ) selected_exemplars = self . _extract_exemplar_subset ( cluster_embeddings , exemplar_embeddings , representative_images ) # Create collective representation of images self . _cluster_representation ( images , selected_exemplars ) # Find the best words for each concept cluster if docs is not None : self . _extract_textual_representation ( cluster_embeddings , docs ) return predictions transform ( self , images , image_embeddings = None ) \u00b6 After having fit a model, use transform to predict new instances Parameters: Name Type Description Default images A single images or a list of images to predict required image_embeddings Pre-trained image embeddings. These can be used instead of the sentence-transformer model. None Returns: Type Description predictions Concept predictions for each image Usage: concept_model = ConceptModel () concepts = concept_model . fit ( images ) new_concepts = concept_model . transform ( new_images ) Source code in concept\\_model.py def transform ( self , images , image_embeddings = None ): \"\"\" After having fit a model, use transform to predict new instances Arguments: images: A single images or a list of images to predict image_embeddings: Pre-trained image embeddings. These can be used instead of the sentence-transformer model. Returns: predictions: Concept predictions for each image Usage: ```python concept_model = ConceptModel() concepts = concept_model.fit(images) new_concepts = concept_model.transform(new_images) ``` \"\"\" if image_embeddings is not None : image_embeddings = self . _embed_images ( images ) umap_embeddings = self . umap_model . transform ( image_embeddings ) predictions , _ = hdbscan . approximate_predict ( self . hdbscan_model , umap_embeddings ) return predictions visualize_concepts ( self , top_n = 9 , concepts = None , figsize = ( 20 , 15 )) \u00b6 Visualize clusters using merged exemplars Parameters: Name Type Description Default top_n int The top_n concepts to visualize 9 concepts List[int] The concept clusters to visualize None figsize Tuple[int, int] The size of the figure (20, 15) Source code in concept\\_model.py def visualize_concepts ( self , top_n : int = 9 , concepts : List [ int ] = None , figsize : Tuple [ int , int ] = ( 20 , 15 )): \"\"\" Visualize clusters using merged exemplars Arguments: top_n: The top_n concepts to visualize concepts: The concept clusters to visualize figsize: The size of the figure \"\"\" if not concepts : clusters = [ self . frequency . index [ index ] for index in range ( top_n )] images = [ self . cluster_images [ index ] for index in clusters ] else : images = [ self . cluster_images [ index ] for index in concepts ] nr_columns = 3 if len ( images ) >= 3 else len ( images ) nr_rows = int ( np . ceil ( len ( concepts ) / nr_columns )) _ , axs = plt . subplots ( nr_rows , nr_columns , figsize = figsize ) axs = axs . flatten () for index , ax in enumerate ( axs ): if index < len ( images ): ax . imshow ( images [ index ]) if self . topics : title = f \"Concept { concepts [ index ] } : { self . topics [ concepts [ index ]] } \" else : title = f \"Concept { concepts [ index ] } \" ax . set_title ( title ) ax . axis ( 'off' ) plt . show ()","title":"Concept"},{"location":"api/concept.html#concept","text":"Concept is a technique that leverages CLIP and BERTopic-based techniques to perform Concept Modeling on images. Since topics are part of conversations and text, they do not represent the context of images well. Therefore, these clusters of images are referred to as 'Concepts' instead of the traditional 'Topics'. Thus, Concept Modeling takes inspiration from topic modeling techniques to cluster images, find common concepts and model them both visually using images and textually using topic representations. Usage: from concept import ConceptModel concept_model = ConceptModel () concept_clusters = concept_model . fit_transform ( images )","title":"Concept"},{"location":"api/concept.html#concept._model.ConceptModel.__init__","text":"Concept Model Initialization Parameters: Name Type Description Default min_concept_size int The minimum size of concepts. Increasing this value will lead to a lower number of concept clusters. 30 diversity float How diverse the images within a concept are. Values between 0 and 1 with 0 being not diverse at all and 1 being most diverse. 0.3 embedding_model str The CLIP model to use. Current options include: * clip-ViT-B-32 * clip-ViT-B-32-multilingual-v1 'clip-ViT-B-32' vectorizer_model CountVectorizer Pass in a CountVectorizer instead of the default None umap_model UMAP Pass in a UMAP model to be used instead of the default None hdbscan_model HDBSCAN Pass in a hdbscan.HDBSCAN model to be used instead of the default None Source code in concept\\_model.py def __init__ ( self , min_concept_size : int = 30 , diversity : float = 0.3 , embedding_model : str = \"clip-ViT-B-32\" , vectorizer_model : CountVectorizer = None , umap_model : UMAP = None , hdbscan_model : hdbscan . HDBSCAN = None ): \"\"\" Concept Model Initialization Arguments: min_concept_size: The minimum size of concepts. Increasing this value will lead to a lower number of concept clusters. diversity: How diverse the images within a concept are. Values between 0 and 1 with 0 being not diverse at all and 1 being most diverse. embedding_model: The CLIP model to use. Current options include: * clip-ViT-B-32 * clip-ViT-B-32-multilingual-v1 vectorizer_model: Pass in a CountVectorizer instead of the default umap_model: Pass in a UMAP model to be used instead of the default hdbscan_model: Pass in a hdbscan.HDBSCAN model to be used instead of the default \"\"\" self . diversity = diversity self . min_concept_size = min_concept_size # Embedding model self . embedding_model = SentenceTransformer ( embedding_model ) # Vectorizer self . vectorizer_model = vectorizer_model or CountVectorizer () # UMAP self . umap_model = umap_model or UMAP ( n_neighbors = 15 , n_components = 5 , min_dist = 0.0 , metric = 'cosine' ) # HDBSCAN self . hdbscan_model = hdbscan_model or hdbscan . HDBSCAN ( min_cluster_size = self . min_concept_size , metric = 'euclidean' , cluster_selection_method = 'eom' , prediction_data = True ) self . frequency = None self . topics = None","title":"__init__()"},{"location":"api/concept.html#concept._model.ConceptModel.fit","text":"Fit the model on a collection of images and return concepts Parameters: Name Type Description Default images List[PIL.Image.Image] A list of images to fit the model on required image_names List[str] The names of the images for easier reading of concept clusters None image_embeddings ndarray Pre-trained image embeddings to use instead of generating them in Concept None Usage: from concept import ConceptModel concept_model = ConceptModel () concepts = concept_model . fit ( images ) Source code in concept\\_model.py def fit ( self , images : List [ Image . Image ], image_names : List [ str ] = None , image_embeddings : np . ndarray = None ): \"\"\" Fit the model on a collection of images and return concepts Arguments: images: A list of images to fit the model on image_names: The names of the images for easier reading of concept clusters image_embeddings: Pre-trained image embeddings to use instead of generating them in Concept Usage: ```python from concept import ConceptModel concept_model = ConceptModel() concepts = concept_model.fit(images) ``` \"\"\" self . fit_transform ( images , image_names = image_names , image_embeddings = image_embeddings ) return self","title":"fit()"},{"location":"api/concept.html#concept._model.ConceptModel.fit_transform","text":"Fit the model on a collection of images and return concepts Parameters: Name Type Description Default images List[PIL.Image.Image] A list of images to fit the model on required docs List[str] The documents from which to extract textual concept representation None image_names List[str] The names of the images for easier reading of concept clusters None image_embeddings ndarray Pre-trained image embeddings to use instead of generating them in Concept None Returns: Type Description predictions Concept prediction for each image Usage: from concept import ConceptModel concept_model = ConceptModel () concepts = concept_model . fit_transform ( images ) Source code in concept\\_model.py def fit_transform ( self , images : List [ Image . Image ], docs : List [ str ] = None , image_names : List [ str ] = None , image_embeddings : np . ndarray = None ) -> List [ int ]: \"\"\" Fit the model on a collection of images and return concepts Arguments: images: A list of images to fit the model on docs: The documents from which to extract textual concept representation image_names: The names of the images for easier reading of concept clusters image_embeddings: Pre-trained image embeddings to use instead of generating them in Concept Returns: predictions: Concept prediction for each image Usage: ```python from concept import ConceptModel concept_model = ConceptModel() concepts = concept_model.fit_transform(images) ``` \"\"\" # Calculate image embeddings if not already generated if image_embeddings is None : image_embeddings = self . _embed_images ( images ) # Reduce dimensionality and cluster images into concepts reduced_embeddings = self . _reduce_dimensionality ( image_embeddings ) predictions = self . _cluster_embeddings ( reduced_embeddings ) # Extract representative images through exemplars representative_images = self . _extract_exemplars ( image_names ) cluster_embeddings , exemplar_embeddings = self . _extract_cluster_embeddings ( image_embeddings , representative_images ) selected_exemplars = self . _extract_exemplar_subset ( cluster_embeddings , exemplar_embeddings , representative_images ) # Create collective representation of images self . _cluster_representation ( images , selected_exemplars ) # Find the best words for each concept cluster if docs is not None : self . _extract_textual_representation ( cluster_embeddings , docs ) return predictions","title":"fit_transform()"},{"location":"api/concept.html#concept._model.ConceptModel.transform","text":"After having fit a model, use transform to predict new instances Parameters: Name Type Description Default images A single images or a list of images to predict required image_embeddings Pre-trained image embeddings. These can be used instead of the sentence-transformer model. None Returns: Type Description predictions Concept predictions for each image Usage: concept_model = ConceptModel () concepts = concept_model . fit ( images ) new_concepts = concept_model . transform ( new_images ) Source code in concept\\_model.py def transform ( self , images , image_embeddings = None ): \"\"\" After having fit a model, use transform to predict new instances Arguments: images: A single images or a list of images to predict image_embeddings: Pre-trained image embeddings. These can be used instead of the sentence-transformer model. Returns: predictions: Concept predictions for each image Usage: ```python concept_model = ConceptModel() concepts = concept_model.fit(images) new_concepts = concept_model.transform(new_images) ``` \"\"\" if image_embeddings is not None : image_embeddings = self . _embed_images ( images ) umap_embeddings = self . umap_model . transform ( image_embeddings ) predictions , _ = hdbscan . approximate_predict ( self . hdbscan_model , umap_embeddings ) return predictions","title":"transform()"},{"location":"api/concept.html#concept._model.ConceptModel.visualize_concepts","text":"Visualize clusters using merged exemplars Parameters: Name Type Description Default top_n int The top_n concepts to visualize 9 concepts List[int] The concept clusters to visualize None figsize Tuple[int, int] The size of the figure (20, 15) Source code in concept\\_model.py def visualize_concepts ( self , top_n : int = 9 , concepts : List [ int ] = None , figsize : Tuple [ int , int ] = ( 20 , 15 )): \"\"\" Visualize clusters using merged exemplars Arguments: top_n: The top_n concepts to visualize concepts: The concept clusters to visualize figsize: The size of the figure \"\"\" if not concepts : clusters = [ self . frequency . index [ index ] for index in range ( top_n )] images = [ self . cluster_images [ index ] for index in clusters ] else : images = [ self . cluster_images [ index ] for index in concepts ] nr_columns = 3 if len ( images ) >= 3 else len ( images ) nr_rows = int ( np . ceil ( len ( concepts ) / nr_columns )) _ , axs = plt . subplots ( nr_rows , nr_columns , figsize = figsize ) axs = axs . flatten () for index , ax in enumerate ( axs ): if index < len ( images ): ax . imshow ( images [ index ]) if self . topics : title = f \"Concept { concepts [ index ] } : { self . topics [ concepts [ index ]] } \" else : title = f \"Concept { concepts [ index ] } \" ax . set_title ( title ) ax . axis ( 'off' ) plt . show ()","title":"visualize_concepts()"}]}